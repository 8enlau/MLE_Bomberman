# Bomberman Agent Repository

This repository contains multiple agents developed to play Bomberman. Below is a detailed description of the structure and purpose of each folder and file. 
Based on the bomberman framework given in https://github.com/ukoethe/bomberman_rl .

## Repository Structure

### 1. Agents

- **`agent_code/`**  
  All the agents are stored in this directory.
  1. **`NeuralNetworkBestAgent/`**  
     Contains the best-performing agent that uses a neural network.
  2. **`NeuralNetwork_AgentHistory/`**  
     This folder contains all other agents developed using the neural network approach.

### 2. Neural Network Approach

- **`neural_network/`**  
  This folder contains all the code related to our neural network-based approach. Below is the structure of the folder and the role of its sub-components:

  1. **`Dataset/`**  
     Contains the datasets used for training the neural network agents. Each file in this folder consists of a list of lists:
     - The outer list represents the game rounds.
     - The inner lists represent the game states for each step, stored as dictionaries.

  2. **`GameLogs/` and `agent_code/`**  
     These folders store log files generated by the Bomberman framework during gameplay.

  3. **`create_Datasets/`**  
     Contains a slightly adapted version of the Bomberman framework, stripped of GUI features and `argparse`. This script is used to generate the data stored in the `Dataset/` folder.

  4. **`TRAIN*/`** (e.g., `TRAIN_6Conv`, `TRAIN_ThreeConvolutional`, etc.)  
     These folders store network layouts and training progress for each iteration of agent development.
     - Each folder contains a `networkLayout.py` file, which defines the neural network structure and the function that rewrites game states from a dictionary format into a 17x17 matrix format for the network.

  5. **`trainingHandler.py`**  
     Manages the optimization of the agents. It uses the `main.py` script in `create_Datasets/` to play games, stores them in the `Dataset/` folder, and then optimizes the agent specified in the `config.yaml` file.

  6. **`config.yaml`**  
     Contains essential configurations for training, such as optimization hyperparameters and the agent's name to be optimized.

  7. **`rewards.py`**  
     Defines the reward function used by the neural network agent to evaluate its performance during training.

  8. **`helperFunctions.py`**  
     Includes functions that assist the reward function and other tasks in the neural network approach.

### 2. Q-Learning Approach

- **`GUI-template-based/`**  
  This folder contains all the code from the bomberman_rl framework provided in the project description, with the following additions and modifications:

  1. **`agent_code/`**  
      Contains the code of select default agents that were used, and our created agents.
      - Modifications to default agents:
         a. `coin_collector_agent`: had the `end_of_round_game` function added for graphing.
         b. `rule_based_agent`: same as above.
      - Our agents
         a. `tpl_based_agent`: first experimentation and understanding of the code. Also containg graphing function, data, and multiple saved model versions.
         b. `my_coin_agent`: first agent with the goal of efficient navigation and coin collection. Also containg graphing function, data,  and multiple saved model versions.
         c. `my_box_agent`: agent with the goal of efficient coin collection but with boxes that must be destroyed, without the agent killing itself.
         d. `the-rolling-bomber`: final submission without the `end_of_round_game` function (as this would cause an error in the competition)
   
   2. **`graph-generator`**
      Contains the code, data and images used when graphing a 4-player competition for the report.

   3. **Remaining subdirectories**
      Are identical to the framework and are included so that the agent can be run with GUI.

   4. **Modified Files**
      - `settings.py`: Custom few-crates scenario added.
      - `environment.py`: Lines 495-497, end of round events are also sent in game, not just training, for graphing purposes.
      - `agents.py`: Added "end_of_round_game" API and modified the round_ended() function to use this in game.

---
